{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”week 1â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "ğŸ“ Course Overview: Building AI Agents\n",
    "ğŸ“š What Youâ€™ll Learn\n",
    "This course focuses on engineering AI agents through a structured, hands-on curriculum that spans six weeks. Each week builds on the last, combining theory, frameworks, and practical projects.\n",
    "\n",
    "ğŸ§  Week-by-Week Breakdown\n",
    "Week 1: Foundations\n",
    "* Understand agentic architectures\n",
    "* Use LLMs to build basic agents without frameworks\n",
    "* Final project: Build a personal career agent for your website\n",
    "Week 2: OpenAI Agents SDK\n",
    "* Learn the OpenAI Agents SDK\n",
    "* Implement guardrails\n",
    "* Build the \"Deep Research\" app\n",
    "Week 3: CrewAI\n",
    "* Low-code tool to configure agent teams\n",
    "* Multiple projects to explore use cases\n",
    "Week 4: LangGraph\n",
    "* Full-code, powerful and complex framework\n",
    "* Tackle more advanced agent workflows\n",
    "Week 5: Microsoft Autogen\n",
    "* Agent collaboration environment\n",
    "* Explore its modular ecosystem\n",
    "Week 6: Capstone & MCP (Model Context Protocol)\n",
    "* Introduce MCP from Anthropic â€” for multi-model collaboration\n",
    "* Final capstone project ties together all learnings\n",
    "\n",
    "ğŸ’¡ Major Projects\n",
    "Projects get progressively more sophisticated:\n",
    "1. Career Agent (Week 1)\n",
    "2. Deep Research App (Week 2)\n",
    "3. Multi-agent collaborations (CrewAI)\n",
    "4. LangGraph applications\n",
    "5. Engineering Team Simulation: Agents simulate dev roles (front-end, back-end, etc.)\n",
    "6. Sidekick Agent: Local browser-based assistant\n",
    "7. Creator Agent: Agents that build other agents\n",
    "8. Trading Simulation: Agents search news, read reports, and simulate trading\n",
    "\n",
    "ğŸ¯ Goals\n",
    "* Educational: Learn core concepts and skills around agents\n",
    "* Commercial: Apply what you build in real-world scenarios, B2B or B2C\n",
    "\n",
    "\n",
    "ğŸ§± What Are Agentic AI Frameworks?\n",
    "* Frameworks simplify building AI agents by abstracting away complex interactions with LLMs (e.g., prompt orchestration, tool use).\n",
    "* Goal: Let developers focus on solving business problems, not wiring systems together.\n",
    "\n",
    "âš™ï¸ Framework Landscape (by Complexity)\n",
    "ğŸŸ¢ 1. No Framework / Minimal Abstraction\n",
    "* Direct API calls to LLMs (like in the lab).\n",
    "* Full control over prompts and logic.\n",
    "* Recommended by Anthropic (e.g., in their Building Effective Agents blog).\n",
    "* Simple, transparent, flexible â€” but manual.\n",
    "â¡ï¸ MCP (Model Context Protocol):\n",
    "* Not a framework, but a protocol by Anthropic.\n",
    "* Enables plug-and-play connections between models, tools, and data sources without glue code.\n",
    "* Open-source, lightweight, standard-based.\n",
    "\n",
    "ğŸŸ¡ 2. Lightweight Frameworks\n",
    "* Good balance of abstraction and control.\n",
    "ğŸ”¹ OpenAI Agents SDK\n",
    "* Simple, clean, new, developer-friendly.\n",
    "* Great for hands-on use with LLMs.\n",
    "* Ideal for small teams or rapid prototyping.\n",
    "ğŸ”¹ Crew AI\n",
    "* Supports collaborative multi-agent systems.\n",
    "* Has a low-code angle (YAML config).\n",
    "* Slightly more â€œheavyâ€ than OpenAI SDK, but intuitive.\n",
    "\n",
    "ğŸ”´ 3. Heavyweight Frameworks\n",
    "* More powerful but complex â€” offer full ecosystems.\n",
    "ğŸ”¸ LangGraph (from LangChain team)\n",
    "* Build agents as computational graphs.\n",
    "* High flexibility for complex workflows.\n",
    "* Comes with steep learning curve and deep ecosystem buy-in.\n",
    "ğŸ”¸ AutoGen (from Microsoft)\n",
    "* Versatile; actually multiple tools in one.\n",
    "* Best for structured multi-agent conversations and complex automation.\n",
    "* Powerful, but requires investment in concepts and setup.\n",
    "\n",
    "ğŸ“Œ How to Choose a Framework\n",
    "* Depends on:\n",
    "    * Use case complexity\n",
    "    * Required features (e.g., state, collaboration)\n",
    "    * Team skill set\n",
    "    * Preference for control vs abstraction\n",
    "* Instructorâ€™s bias: lightweight tools that â€œstay out of your way,â€ but acknowledges the power of advanced frameworks.\n",
    "\n",
    "ğŸ§  Resources\n",
    "* Definition: Extra contextual information provided to an LLM to make it smarter or more useful.\n",
    "* Example: Feeding an airline support agent LLM with ticket pricing data so it can answer questions accurately.\n",
    "âœ… Key Ideas:\n",
    "* Basic approach: Just add relevant info directly into the prompt.\n",
    "* Smarter approach: Use RAG (Retrieval-Augmented Generation) to fetch only relevant context.\n",
    "    * This avoids overloading the prompt and improves focus.\n",
    "    * Often uses other LLMs to assist in filtering or selecting content.\n",
    "\n",
    "ğŸ› ï¸ Tools\n",
    "* Definition: Capabilities the LLM can choose to use to perform real-world actions, like querying a database or sending a message.\n",
    "* Purpose: Give the LLM autonomy to act, not just respond.\n",
    "ğŸ” How It Works (in reality):\n",
    "* Itâ€™s not magic â€” itâ€™s just structured logic:\n",
    "    * You define tools and their purposes in the prompt.\n",
    "    * The LLM replies with a structured response (usually JSON) saying what tool to use and with what parameters.\n",
    "    * Your code (not the LLM) detects this and executes the tool.\n",
    "    * Then, you call the LLM again, including the result of the tool's output.\n",
    "ğŸ§ª Example:\n",
    "text\n",
    "CopyEdit\n",
    "Prompt: â€œYou can use 'fetch_ticket_price'. If needed, reply with JSON.â€\n",
    "User: â€œI want to fly to Paris.â€\n",
    "LLM: { \"tool\": \"fetch_ticket_price\", \"destination\": \"Paris\" }\n",
    "â†’ You run the tool â†’ re-prompt the LLM with the result.\n",
    "\n",
    "ğŸ”„ Tools vs. Resources\n",
    "Feature\tResources\tTools\n",
    "Role\tProvide information\tExecute actions\n",
    "How Used\tInjected into prompt\tCalled via structured LLM responses\n",
    "Autonomy Level\tLow to medium (contextual assistance)\tHigh (lets the agent do something)\n",
    "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-Week 2â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "âš¡ Why Learn Asyncio?\n",
    "* Async Python is used across all agent frameworks (OpenAI SDK, CrewAI, LangGraph, etc.).\n",
    "* Helps agents handle many concurrent tasks (e.g., multiple LLM API calls) efficiently.\n",
    "* Especially useful for I/O-bound operations, such as:\n",
    "    * Waiting on LLM responses\n",
    "    * Fetching web data\n",
    "    * Querying databases or tools\n",
    "\n",
    "ğŸ§µ What Is Asyncio?\n",
    "* Lightweight concurrency model in Python (introduced in Python 3.5).\n",
    "* Not multithreading or multiprocessing â€” avoids OS-level threads.\n",
    "* Runs a single-threaded event loop that switches between tasks when they pause (e.g., waiting for I/O).\n",
    "\n",
    "ğŸ”‘ Core Concepts\n",
    "âœ… async def\n",
    "* Declares a coroutine (not a regular function).\n",
    "* It wonâ€™t run immediatelyâ€”returns a coroutine object.\n",
    "âœ… await\n",
    "* Tells Python to pause and wait for the coroutine to complete.\n",
    "* Example:â€¨pythonâ€¨CopyEditâ€¨â€¨â€¨async def fetch_data(): \n",
    "*     return \"done\"\n",
    "* \n",
    "* result = await fetch_data()\n",
    "* â€¨â€¨\n",
    "âœ… Coroutine\n",
    "* A function that can pause and resume during execution.\n",
    "* Python's event loop schedules and manages these coroutines.\n",
    "\n",
    "ğŸŒ€ The Event Loop\n",
    "* Core engine that:\n",
    "    * Starts and pauses coroutines\n",
    "    * Runs other coroutines while one is waiting\n",
    "    * Enables concurrent behavior in I/O-heavy programs\n",
    "\n",
    "ğŸ› ï¸ Useful Pattern: asyncio.gather()\n",
    "* Run multiple coroutines concurrently.\n",
    "* Example:â€¨pythonâ€¨CopyEditâ€¨â€¨â€¨results = await asyncio.gather(\n",
    "*     fetch_data1(), fetch_data2(), fetch_data3()\n",
    "* )\n",
    "* â€¨â€¨\n",
    "* Efficient when calling multiple APIs or agents at once.\n",
    "\n",
    "ğŸ§  How to Think About It\n",
    "* Async Python = manual multitasking at the code level.\n",
    "* Great for multi-agent orchestration, background tasks, and responsiveness.\n",
    "\n",
    "ğŸ“Œ Bottom Line\n",
    "Concept\tMeaning\n",
    "async def\tDefine a coroutine\n",
    "await\tRun the coroutine and wait for result\n",
    "Coroutine\tPauseable, resumable function\n",
    "Event loop\tOrchestrates coroutine execution\n",
    "asyncio.gather\tRun multiple coroutines concurrently\n",
    "\n",
    "\n",
    "ğŸš€ OpenAI Agents SDK: Overview\n",
    "* Lightweight, flexible, and not opinionated â€” lets you design agents your way.\n",
    "* Simplifies common tasks like tool use and LLM orchestration (e.g., managing JSON, if-statements).\n",
    "* Great for rapid prototyping without getting bogged down in boilerplate.\n",
    "\n",
    "âš™ï¸ Why Use It?\n",
    "* Handles routine complexity like:\n",
    "    * Tool invocation structure\n",
    "    * Multi-step agent coordination\n",
    "    * Prompt formatting & response parsing\n",
    "* Makes tool usage clean and maintainable without losing control over the logic.\n",
    "\n",
    "ğŸ“š Key Terminology\n",
    "Term\tMeaning\n",
    "Agent\tA defined role + behavior built around LLM calls\n",
    "Handoff\tInteraction or communication between agents\n",
    "Guardrails\tChecks and constraints to ensure agents stay on-task & safe\n",
    "\n",
    "ğŸ§ª How to Use It: The 3-Step Pattern\n",
    "1. Create an agent instanceâ€¨â†’ Define what role the agent plays (e.g., researcher, planner, responder).\n",
    "2. Use with trace blockâ€¨â†’ Optional but recommended for debugging and logging interactions using OpenAIâ€™s trace viewer.\n",
    "3. Run the agent with runner.run()â€¨â†’ This is a coroutine â†’ must use await.\n",
    "ğŸ” Typical Code Flow\n",
    "python\n",
    "CopyEdit\n",
    "async def main():\n",
    "    agent = Agent(role=..., instructions=...)\n",
    "    with trace(agent):\n",
    "        result = await runner.run(agent)\n",
    "\n",
    "\n",
    "ğŸ§ What Is Vibe Coding?\n",
    "Coined by Andrej Karpathy, vibe coding is a relaxed, iterative way of coding with LLMs â€” generating snippets, tweaking them, and building up functionality quickly without overplanning.\n",
    "\n",
    "âœ… Why Itâ€™s Powerful\n",
    "* Boosts creativity and momentum.\n",
    "* Lets you explore and learn new frameworks or APIs quickly.\n",
    "* Perfect for prototyping and experimenting.\n",
    "\n",
    "ğŸ›¡ï¸ 5 Survival Tips for Effective Vibe Coding\n",
    "1. Good Vibes\n",
    "* Craft high-quality prompts you can reuse.\n",
    "* Ask for concise, modern code (LLMs can be verbose or outdated).\n",
    "* Include todayâ€™s date to get up-to-date API usage.\n",
    "2. Vibe but Verify\n",
    "* Donâ€™t trust one model blindly.\n",
    "* Cross-check outputs by asking the same question to multiple LLMs (e.g., ChatGPT & Claude).\n",
    "3. Step Up the Vibe\n",
    "* Avoid giant blobs of LLM-generated code.\n",
    "* Ask for code in small, testable chunks (e.g., one function at a time).\n",
    "* Not sure how to break it down? Ask the LLM to design the step-by-step breakdown first.\n",
    "4. Vibe and Validate\n",
    "* Use a second LLM to review or optimize what the first LLM wrote.\n",
    "* Ask for feedback: \"Is this the cleanest/best way to do it?\"\n",
    "* Emulates the evaluator-optimizer agent pattern manually.\n",
    "5. Vibe with Variety\n",
    "* Ask for multiple solutions to the same problem.\n",
    "* Encourage creativity and comparison.\n",
    "* Request explanations to deepen your understanding and catch flaws.\n",
    "\n",
    "ğŸ’¬ Final Thought\n",
    "Vibe coding is fun only if you understand what's going on.\n",
    "Always follow up by asking the LLM to explain the code until itâ€™s fully clear to you. Otherwise, debugging becomes painful fast.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
